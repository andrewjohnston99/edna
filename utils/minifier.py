#!/usr/bin/python3
# -*- coding: utf-8 -*-
"""Raw .jsonl.gz minifier

This script is a minifier for raw .jsonl.gz files generated by a python hydrator.
It removes a lot of unneeded components from the raw tweet object. We still backup
the original raw tweets. The minification is primarily for our development environment.

For example, the minifier removes the 'text' field from the tweet object, since
'text' only contains a shortened form and the 'full_text' field actually contains
the full text field.

Example:

        $ python converter.py  --dir 2020-01 --skip 0

Args:
    dir (str): The directory to traverse. This is a directory with a bunch of `.jsonl.gz` files.
        It will generate a minified file with the extension `.jsonl.min.gz`
    skip (int): Whether the skip the most recently generated `.jsonl.gz` file. This is if you execute this
        while the `.jsonl.gz`. files are being generated (only useful for our development environment)


Todo:
    * 
    
"""
# $ zcat ebka-covid-tweets-2020-07-01-01.json.gz > ebka-covid-tweets-2020-07-01-01.json

# Imports
import click, os.path as osp, pdb, gzip, json
from glob import glob as glob
from datetime import datetime, timedelta
from email.utils import parsedate_tz
from tqdm import tqdm


def to_datetime(datestring):
    """Convert a date string from Twitter to a datetime

    Args:
        datestring (str): The datestring from a tweet object, specifically the `created_at` field

    Returns:
        datetime.datetime: Returns a datetime.datetime object
    """
    time_tuple = parsedate_tz(datestring.strip())
    dt = datetime(*time_tuple[:6])
    return dt - timedelta(seconds=time_tuple[-1])

def _reader_generator(reader):
    """Generator for reading a file for python open.

    Args:
        reader (open() object): The python open() object

    Returns:
        The yields some data from the reader
    """
    b = reader(1024 * 1024)
    while b:
        yield b
        b = reader(1024 * 1024)
def file_line_count(fname):
    """Counts number of lines in file

    Args:
        fname(str): Name of the file

    Returns:
        int: Numebr of lines in a file
    """
    f = open(fname, 'rb')
    f_gen = _reader_generator(f.raw.read)
    return sum(buf.count(b'\n') for buf in f_gen)

@click.command()
@click.option('--dir', prompt='Directory to traverse', help='The directory to traverse')   
@click.option('--skip', prompt='Skip the last?', help='1 means skip the last, 0 means no skip, default 1', default=1, type=int) 
def main(dir, skip):
    """Main minifier script

    This script is a minifier for raw .jsonl.gz files generated by a python hydrator. 
    It removes a lot of unneeded components from the raw tweet object. We still backup 
    the original raw tweets. The minification is primarily for our development environment.

    For example, the minifier removes the 'text' field from the tweet object, since
    'text' only contains a shortened form and the 'full_text' field actually contains
    the full text field.

    Args:
        dir (str): The directory to traverse. This is a directory with a bunch 
            of `.jsonl.gz` files. It will generate a minified file with the extension `.jsonl.min.gz`
        skip (int): Whether the skip the most recently generated `.jsonl.gz` file. 
            This is if you execute this while the `.jsonl.gz`. files are being 
            generated (only useful for our development environment)

    """
    remove_keywords = []

    # Get the source files, ordered by their edit date, descending (so that we can skip if needed)
    compressed_files_wildcard = osp.join(dir,"*.jsonl.gz")
    compressed_files_list = glob(compressed_files_wildcard)
    compressed_files_list.sort(key=osp.getmtime)
    assert(skip == 0 or skip == 1)	# Basic sanity check

    # Skip most recent file, if needed
    for compressed_file in compressed_files_list[:len(compressed_files_list)-skip]:
        # Get the output file name
        uncompressed_output_file = compressed_file.replace(".jsonl.gz", ".jsonl.min.gz")
        if osp.exists(uncompressed_output_file):
            print("Skipping {file}; {out} already exists.".format(file=compressed_file, out=uncompressed_output_file))
            continue 
        line_count = file_line_count(compressed_file)
        print("Processing {file} with {lines} lines, writing to {out}.".format(file=compressed_file, lines=line_count, out=uncompressed_output_file))
        
        # Open the raw compressed file and the minified compressed file to write
        with gzip.open(compressed_file, "r") as compressed_gzip_reader:
            with gzip.open(uncompressed_output_file, "w") as compressed_gzip_writer:
                # Process each line -- keep only necessary fields
                for line in tqdm(compressed_gzip_reader, miniters=100,unit="lines"):
                    try:
                        json_line = json.loads(line.decode("utf-8").strip())
                        json_final = {}
                        # If there are any skip keywords (see line 99 (sp?))
                        if len([item for item in remove_keywords if item in json_line["full_text"]]):
                            continue
                        
                        # Remove uneeded components
                        primary = ["coordinates", "created_at", "favorite_count","favorited","geo","id","id_str","in_reply_to_screen_name","in_reply_to_status_id_str","in_reply_to_user_id_str","is_quote_status","lang","place","source","retweet_count","retweeted","full_text"]
                        for field in primary:
                            if field in json_line:
                                json_final[field] = json_line[field]
                        
                        users_fields = ["favourites_count","follow_request_sent","followers_count","following","friends_count","geo_enabled","lang","listed_count","location","name","id_str","screen_name","statuses_count","time_zone"]
                        json_final["user"]={}
                        for field in users_fields:
                            if field in json_line:
                                json_final["user"][field] = json_line["user"][field]
                        
                        if "retweeted_status" in json_line:
                            retweet = ["favorites_count","id_str","retweet_count","text","user","id_str","screen_name"]
                            json_final["retweeted_status"]={}
                            for field in retweet:
                                if field in json_line:
                                    json_final["retweeted_status"][field] = json_line["retweeted_status"][field]

                        if "quoted_status" in json_line:
                            quoted_status = ["favorite_count","id_str","retweet_count","text"]
                            json_final["quoted_status"]={}
                            for field in quoted_status:
                                if field in json_line:
                                    json_final["quoted_status"][field] = json_line["quoted_status"][field]
                        
                        json_final["timestamp_ms"] = int(to_datetime(json_final["created_at"]).timestamp())
                        #
                        compressed_gzip_writer.write(json.dumps(json_final).encode('utf8') + b"\n")

                    except ValueError as e:
                        pass
                    except KeyError as e:
                        pass


if __name__ == "__main__":
    main()
